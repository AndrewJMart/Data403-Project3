{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Whole Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "\n",
    "# Define transformations (resize, normalize, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Load the dataset (class labels are inferred from folder names)\n",
    "full_dataset = datasets.ImageFolder('./403-Project3-Dataset/Dataset', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 485\n",
      "Class names: ['Alex', 'Kelly']\n",
      "Class distribution: {'Alex': 256, 'Kelly': 229}\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of images: {len(full_dataset)}')\n",
    "print(f'Class names: {full_dataset.classes}')\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Get all the labels (targets)\n",
    "targets = [label for _, label in full_dataset]\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = Counter(targets)\n",
    "\n",
    "# Map class indices to class names\n",
    "class_names = full_dataset.classes\n",
    "class_distribution = {class_names[idx]: count for idx, count in class_counts.items()}\n",
    "\n",
    "print(f'Class distribution: {class_distribution}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating K Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # CONVL 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16) \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.25)  \n",
    "\n",
    "        # CONVL 2\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)  \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.25)  \n",
    "        \n",
    "        # CONVL 3\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)  \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout3 = nn.Dropout(0.25)  \n",
    "\n",
    "        # MLP Layers\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 128)  # Adjust size according to input dimensions\n",
    "        self.dropout_fc1 = nn.Dropout(0.5)  # Dropout in FC Layer\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 classes for output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = x.view(-1, 256 * 14 * 14)  # Flatten for fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 13.48855994428907\n",
      "Epoch 2, Loss: 6.4497054644993375\n",
      "Epoch 3, Loss: 2.495854616165161\n",
      "Epoch 4, Loss: 2.0422814914158414\n",
      "Epoch 5, Loss: 1.0848817144121443\n",
      "Validation Accuracy for Fold 1: 68.04%\n",
      "Fold 2\n",
      "Epoch 1, Loss: 7.043987887246268\n",
      "Epoch 2, Loss: 4.280316097395761\n",
      "Epoch 3, Loss: 1.4477314502000809\n",
      "Epoch 4, Loss: 1.0156748976026262\n",
      "Epoch 5, Loss: 0.6173615796225411\n",
      "Validation Accuracy for Fold 2: 68.04%\n",
      "Fold 3\n",
      "Epoch 1, Loss: 16.547843422208512\n",
      "Epoch 2, Loss: 1.5998685785702296\n",
      "Epoch 3, Loss: 0.6823646596499852\n",
      "Epoch 4, Loss: 0.6831548299108233\n",
      "Epoch 5, Loss: 0.7262020196233477\n",
      "Validation Accuracy for Fold 3: 48.45%\n",
      "Fold 4\n",
      "Epoch 1, Loss: 16.86480869565691\n",
      "Epoch 2, Loss: 5.416777150971549\n",
      "Epoch 3, Loss: 2.713440581623997\n",
      "Epoch 4, Loss: 1.7170007739748274\n",
      "Epoch 5, Loss: 0.7727605317320142\n",
      "Validation Accuracy for Fold 4: 61.86%\n",
      "Fold 5\n",
      "Epoch 1, Loss: 10.086397920336042\n",
      "Epoch 2, Loss: 1.8452890472752708\n",
      "Epoch 3, Loss: 0.9130093966211591\n",
      "Epoch 4, Loss: 0.603099490915026\n",
      "Epoch 5, Loss: 0.5910787752696446\n",
      "Validation Accuracy for Fold 5: 59.79%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(full_dataset)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    \n",
    "    # Create data loaders for train and validation sets\n",
    "    train_subset = Subset(full_dataset, train_idx)\n",
    "    val_subset = Subset(full_dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Initialize the model, optimizer, and learning rate scheduler\n",
    "    model = CNNModel()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(5):  # Number of epochs per fold\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch + 1} | Loss: {running_loss / len(train_loader):.4f} | Training Accuracy: {train_accuracy:.2f}%')\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Validation Accuracy for Fold {fold + 1}: {100 * correct / total:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
